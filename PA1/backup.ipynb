{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchGD(learning_rate, t_N, x_N, y_N, w):\n",
    "    size_N = len(x_N)\n",
    "#     for i in range(epoch_size):\n",
    "#         a = np.zeros(len(w))\n",
    "#         for i in range(N):\n",
    "#             a += -(t_N[ci]-y_N[i])*x_N[i]\n",
    "\n",
    "    w = w + learning_rate/size_N*np.sum([np.dot(t_N[j]-y_N[j],x_N[j]) for j in range(size_N)], axis = 0)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchGD_softmax(learning_rate, t_kn, x_kn, y_kn, w):\n",
    "    size_N = len(x_kn)\n",
    "    c = len(w[0])\n",
    "    d = len(w)\n",
    "\n",
    "\n",
    "    sum = np.zeros([d, c])\n",
    "    for n in range(size_N):\n",
    "        m = [[(t_kn[n][k] - y_kn[n][k])*x_kn[n][j] for k in range(c)] for j in range(d)]\n",
    "#         print(\"m\",m)\n",
    "        sum += m\n",
    "#         print(\"sum\",sum)\n",
    "    w = w + learning_rate/size_N/c*sum\n",
    "#     print(\"BDG\",w)\n",
    "    return w   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StochasticGD_softmax(learning_rate, t_kn, x_kn, y_kn, w):\n",
    "    size_N = len(x_kn)\n",
    "    c = len(w[0])\n",
    "    d = len(w)\n",
    "    P = [i for i in range(size_N)]\n",
    "    random.shuffle(P)\n",
    "    \n",
    "    for n in range(size_N):\n",
    "        m = [[(t_kn[P[n]][k] - y_kn[P[n]][k])*x_kn[P[n]][j] for k in range(c)] for j in range(d)]\n",
    "        w = w + learning_rate/size_N/c*np.array(m)\n",
    "        y_kn = softmax_reg(w, x_kn)\n",
    "#     print(\"SDG\",w)\n",
    "    return w  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_softmax(k_pca,learning_rate,train_x,train_t,holdout_x,holdout_t,test_x,test_t, mode = \"BGD\"):\n",
    "    \n",
    "#     print(\"trainx\", train_x)\n",
    "    e_t = []\n",
    "    e_h = []\n",
    "    e_tstop = 1\n",
    "    e_hstop = 1\n",
    "    w_t = []\n",
    "\n",
    "    w = np.zeros([k_pca+1,len(train_t[0])])\n",
    "    w_t = np.zeros([k_pca+1,len(train_t[0])])\n",
    "    train_y = softmax_reg(w, train_x)\n",
    "    \n",
    "    for i in range(20):\n",
    "       \n",
    "        if mode == \"SGD\":\n",
    "            w = StochasticGD_softmax(learning_rate, train_t, train_x, train_y, w) \n",
    "            print(\"SGD:\", w)\n",
    "        else:\n",
    "            w = batchGD_softmax(learning_rate, train_t, train_x, train_y, w)  \n",
    "            print(\"BGD:\", w)\n",
    "        \n",
    "        train_y = softmax_reg(w, train_x)\n",
    "        e_t += [cross_entropy_softmax(train_t, train_y)]\n",
    "                \n",
    "        holdout_y = softmax_reg(w, holdout_x)\n",
    "        e_h += [cross_entropy_softmax(holdout_t, holdout_y)]\n",
    "\n",
    " \n",
    "        \n",
    "        if e_h[-1] <= e_hstop:\n",
    "            e_hstop = e_h[-1]\n",
    "            w_t = w\n",
    "            e_tstop = e_t[-1]\n",
    "        else:\n",
    "            e_h[-1] = e_hstop\n",
    "            e_t[-1] = e_tstop\n",
    "    \n",
    "    return e_t, e_h, w_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 80 and labels: 80\n",
      "*****1.(b)*****\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****1.(c)*****\n",
      "Save PCA image to ./1c_pca_display.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****2.(b)*****\n",
      "PCA=1 60.0% (0.49)\n",
      "PCA=2 80.0% (0.4)\n",
      "PCA=4 100.0% (0.0)\n",
      "PCA=8 95.0% (0.22)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate=0.01 95.0% (0.22)\n",
      "learning rate=3 95.0% (0.22)\n",
      "learning rate=10 95.0% (0.22)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****2.(c)*****\n",
      "learning rate=3 75.0% (0.43)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****3.(a)*****\n",
      "confusion matrix: \n",
      "[[0.9 0.  0.  0.1 0.  0. ]\n",
      " [0.1 0.2 0.  0.3 0.1 0.3]\n",
      " [0.  0.1 0.6 0.2 0.  0.1]\n",
      " [0.1 0.1 0.2 0.5 0.  0.1]\n",
      " [0.  0.3 0.  0.1 0.5 0.1]\n",
      " [0.2 0.  0.  0.2 0.1 0.5]]\n",
      "confusion matrix: \n",
      "[[0.8 0.  0.  0.2 0.  0. ]\n",
      " [0.1 0.2 0.  0.4 0.2 0.1]\n",
      " [0.  0.1 0.4 0.4 0.  0.1]\n",
      " [0.1 0.1 0.1 0.6 0.1 0. ]\n",
      " [0.1 0.4 0.  0.1 0.3 0.1]\n",
      " [0.2 0.  0.  0.2 0.  0.6]]\n",
      "confusion matrix: \n",
      "[[0.8 0.  0.  0.2 0.  0. ]\n",
      " [0.1 0.1 0.  0.4 0.2 0.2]\n",
      " [0.1 0.1 0.5 0.3 0.  0. ]\n",
      " [0.1 0.1 0.1 0.6 0.1 0. ]\n",
      " [0.1 0.3 0.  0.1 0.3 0.2]\n",
      " [0.1 0.  0.  0.2 0.  0.7]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: \n",
      "[[0.7 0.  0.  0.2 0.  0.1]\n",
      " [0.1 0.2 0.1 0.3 0.3 0. ]\n",
      " [0.  0.  0.5 0.5 0.  0. ]\n",
      " [0.1 0.  0.1 0.7 0.  0.1]\n",
      " [0.  0.2 0.1 0.2 0.5 0. ]\n",
      " [0.4 0.1 0.  0.2 0.1 0.2]]\n",
      "confusion matrix: \n",
      "[[0.7 0.  0.  0.3 0.  0. ]\n",
      " [0.1 0.2 0.  0.3 0.3 0.1]\n",
      " [0.  0.1 0.2 0.6 0.  0.1]\n",
      " [0.  0.  0.1 0.7 0.2 0. ]\n",
      " [0.1 0.2 0.1 0.2 0.4 0. ]\n",
      " [0.3 0.1 0.  0.2 0.  0.4]]\n",
      "confusion matrix: \n",
      "[[0.8 0.  0.  0.2 0.  0. ]\n",
      " [0.1 0.1 0.2 0.3 0.3 0. ]\n",
      " [0.1 0.1 0.4 0.3 0.  0.1]\n",
      " [0.1 0.  0.1 0.7 0.1 0. ]\n",
      " [0.1 0.1 0.1 0.2 0.4 0.1]\n",
      " [0.3 0.  0.1 0.1 0.  0.5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: \n",
      "[[1.  0.  0.  0.  0.  0. ]\n",
      " [0.1 0.1 0.1 0.2 0.3 0.2]\n",
      " [0.1 0.1 0.3 0.3 0.1 0.1]\n",
      " [0.3 0.  0.3 0.4 0.  0. ]\n",
      " [0.1 0.1 0.1 0.2 0.3 0.2]\n",
      " [0.7 0.1 0.1 0.  0.  0.1]]\n",
      "confusion matrix: \n",
      "[[0.7 0.  0.  0.1 0.  0.2]\n",
      " [0.  0.3 0.  0.2 0.3 0.2]\n",
      " [0.  0.1 0.6 0.2 0.  0.1]\n",
      " [0.1 0.1 0.2 0.5 0.  0.1]\n",
      " [0.  0.3 0.  0.  0.5 0.2]\n",
      " [0.1 0.1 0.  0.1 0.2 0.5]]\n",
      "confusion matrix: \n",
      "[[0.8 0.  0.  0.  0.  0.2]\n",
      " [0.  0.2 0.  0.3 0.2 0.3]\n",
      " [0.  0.  0.6 0.3 0.  0.1]\n",
      " [0.1 0.2 0.1 0.5 0.  0.1]\n",
      " [0.  0.1 0.  0.  0.6 0.3]\n",
      " [0.2 0.1 0.  0.1 0.  0.6]]\n",
      "confusion matrix: \n",
      "[[0.7 0.  0.  0.1 0.  0.2]\n",
      " [0.  0.3 0.  0.3 0.2 0.2]\n",
      " [0.  0.  0.5 0.5 0.  0. ]\n",
      " [0.1 0.1 0.1 0.6 0.1 0. ]\n",
      " [0.  0.2 0.  0.1 0.5 0.2]\n",
      " [0.2 0.1 0.  0.1 0.1 0.5]]\n",
      "confusion matrix: \n",
      "[[0.7 0.  0.  0.2 0.  0.1]\n",
      " [0.  0.4 0.  0.3 0.2 0.1]\n",
      " [0.  0.  0.4 0.5 0.  0.1]\n",
      " [0.1 0.1 0.1 0.6 0.1 0. ]\n",
      " [0.  0.4 0.  0.  0.5 0.1]\n",
      " [0.1 0.1 0.  0.1 0.1 0.6]]\n",
      "confusion matrix: \n",
      "[[0.6 0.  0.  0.2 0.  0.2]\n",
      " [0.1 0.3 0.  0.1 0.3 0.2]\n",
      " [0.  0.  0.7 0.3 0.  0. ]\n",
      " [0.1 0.  0.2 0.6 0.1 0. ]\n",
      " [0.  0.2 0.  0.  0.5 0.3]\n",
      " [0.1 0.1 0.  0.  0.1 0.7]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****3.(b)*****\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from dataloader import *\n",
    "from pca import *\n",
    "import random\n",
    "\n",
    "\n",
    "# import data\n",
    "images,labels = load_data()\n",
    "emo_imgs = []\n",
    "    \n",
    "# parse the label of images and remove images labeled with 'neutral' or 'happy', key is subject\n",
    "dict_label = {}\n",
    "emotion = ['m','s','f','a','d']\n",
    "for i in range(len(labels)):\n",
    "    s = labels[i][:-4].split(\"_\")\n",
    "    if s[0] not in dict_label:\n",
    "        dict_label[s[0]] = []\n",
    "    if s[1][:2] == \"ht\" or s[1][0] in emotion:\n",
    "        dict_label[s[0]] += [(s[1], images[i])]\n",
    "\n",
    "# parse the label of images and remove images labeled with 'neutral' or 'happy', key is emotion\n",
    "dict_label_ec = {}\n",
    "emotion = ['m','s','f','a','d']\n",
    "for i in range(len(labels)):\n",
    "    s = labels[i][:-4].split(\"_\")\n",
    "    if s[1][:2] == \"ht\" or s[1][0] in emotion:\n",
    "        if s[1][0] not in dict_label_ec:\n",
    "            dict_label_ec[s[1][0]] = []\n",
    "        dict_label_ec[s[1][0]] += [(s[0], images[i])]\n",
    "\n",
    "\n",
    "# 2.Logistic regression\n",
    "\n",
    "# Choose images labeled e1 or e2 from a subject\n",
    "# build its corresponding expected values\n",
    "def subject_emo(e1,e2,subject,encode, dataset = dict_label):\n",
    "    x_set = []\n",
    "    t = []\n",
    "    for i in subject:\n",
    "        for l,img in dataset[i]:\n",
    "            if l[0] == e1:\n",
    "                x_set += [img]\n",
    "                t += [encode]\n",
    "      \n",
    "            if l[0] == e2:\n",
    "                x_set += [img]\n",
    "                t += [int(not encode)]\n",
    "            \n",
    "    return (np.array(x_set),np.array(t))\n",
    "\n",
    "\n",
    "# Choose a pair of subject as test set\n",
    "# randomly choose a pair of subjects as holdout set from the rest\n",
    "# then the remaining is train set\n",
    "def choose_set(e1, e2, encode, index, dataset = dict_label):\n",
    "    subject = list(dataset.keys())\n",
    "    \n",
    "    test = subject[2*index: 2*index+2]\n",
    "    test_x, test_t = subject_emo(e1,e2,test,encode)\n",
    "    \n",
    "    rest = subject[:2*index] + subject[2*index+2:]\n",
    "    random.shuffle(rest)\n",
    "    \n",
    "    holdout = rest[:2]\n",
    "    holdout_x, holdout_t = subject_emo(e1,e2,holdout,encode)\n",
    "    \n",
    "    train = rest[2:]\n",
    "    train_x, train_t = subject_emo(e1,e2,train,encode)\n",
    "    \n",
    "    return train_x,train_t,holdout_x, holdout_t,test_x, test_t\n",
    "\n",
    "\n",
    "# Compute predicted values for logistic regression\n",
    "def logistic_reg(w, x_N):\n",
    "    y_N = np.array([1/(1+np.exp(-np.dot(w,x))) for x in x_N])\n",
    "    return y_N\n",
    "\n",
    "\n",
    "# Compute loss function for logistic regression\n",
    "def loss_logistics(t_N, y_N):\n",
    "    N = len(t_N)\n",
    "    return sum([-(t_N[n]*np.log(y_N[n]) + (1-t_N[n])*np.log(1-y_N[n])) for n in range(N)])/N\n",
    "\n",
    "\n",
    "# Compute the train error, holdout error and best weight vector for each run\n",
    "def run_logistic(epoch,learning_rate,train_x,train_t,holdout_x,holdout_t,test_x,test_t):\n",
    "    N = len(train_x)\n",
    "    d = len(train_x[0])\n",
    "    \n",
    "    e_t = []\n",
    "    e_h = []\n",
    "    error = 1\n",
    "    w_t = []\n",
    "\n",
    "    # weight vector initialization\n",
    "    w = np.zeros(d)\n",
    "    w_t = np.zeros(d)\n",
    "    \n",
    "    # for each epoch,use batch gradient descent to update weight vector, record error and best weights\n",
    "    for i in range(epoch):\n",
    "\n",
    "        train_y = logistic_reg(w, train_x)\n",
    "        e_t += [loss_logistics(train_t, train_y)]\n",
    "        \n",
    "        w = w + learning_rate/N*np.sum([np.dot(train_t[n]-train_y[n],train_x[n]) for n in range(N)], axis = 0) \n",
    "          \n",
    "        holdout_y = logistic_reg(w, holdout_x)\n",
    "        e_h += [loss_logistics(holdout_t, holdout_y)]\n",
    "\n",
    "        if e_h[-1] <= error:\n",
    "            error = e_h[-1]\n",
    "            w_t = w\n",
    "    \n",
    "    return e_t, e_h, w_t\n",
    "\n",
    "\n",
    "# Plot train error, holdout error for 5 runs with different number of principal components\n",
    "# Calculate corresponding test accuracy\n",
    "def training_proc(epoch,kd,e1,e2,encode,learning_rate):\n",
    "    e_train = {}\n",
    "    e_holdout = {}\n",
    "    e_test = {}\n",
    "\n",
    "    for i in range(5):\n",
    "        # choose train set, holdout set, test set \n",
    "        train_X,train_t,holdout_X, holdout_t,test_X, test_t = choose_set(e1,e2,encode,i)  \n",
    "        \n",
    "        for k_pca in kd:\n",
    "            if k_pca not in e_train:\n",
    "                e_train[k_pca] = []\n",
    "                e_holdout[k_pca] = []\n",
    "                e_test[k_pca] = []\n",
    "                \n",
    "            # transform images to k-dimension\n",
    "            pca = PCA(k = k_pca)\n",
    "            pca.fit(train_X)\n",
    "            train_x = [j[0] for j in [pca.transform(i) for i in train_X]]\n",
    "            holdout_x = [j[0] for j in [pca.transform(i) for i in holdout_X]]\n",
    "            test_x = [j[0] for j in [pca.transform(i) for i in test_X]]\n",
    "\n",
    "            e_t, e_h, w_t = run_logistic(epoch,learning_rate,train_x,train_t,holdout_x,holdout_t,test_x,test_t)\n",
    "\n",
    "            # store train error, holdout error for each run\n",
    "            e_train[k_pca] += [e_t]\n",
    "            e_holdout[k_pca] += [e_h]\n",
    "            \n",
    "            # calculate test accuracy\n",
    "            test_y = logistic_reg(w_t, test_x)\n",
    "            e_test[k_pca] += [(test_y > 0.5) == test_t]\n",
    "            \n",
    "    for k in kd:\n",
    "        plt.figure(kd.index(k))\n",
    "        \n",
    "        # calculate average of train error, holdout error for 10 epochs\n",
    "        avg_train = [np.mean([x[i] for x in e_train[k]]) for i in range(10)]\n",
    "        avg_holdout = [np.mean([x[i] for x in e_holdout[k]]) for i in range(10)]\n",
    "\n",
    "        # calculate std of train error, holdout error for epoch 2,4,6,8\n",
    "        std_train = [np.std([x[i-1] for x in e_train[k]]) for i in [2,4,6,8]]\n",
    "        std_holdout = [np.std([x[i-1] for x in e_holdout[k]]) for i in [2,4,6,8]]\n",
    "        \n",
    "        # plot graphs\n",
    "        x = [i for i in range(1,11)]\n",
    "        plt.plot(x, avg_train, label = \"avg train error\")\n",
    "        plt.plot(x, avg_holdout, label = \"avg holdout error\")\n",
    "        plt.plot([2,4,6,8], std_train, label = \"std train error\")\n",
    "        plt.plot([2,4,6,8], std_holdout, label = \"std holdout error\")\n",
    "        plt.xticks(np.arange(1, 11, step=1))\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Cross-Entropy Loss\")\n",
    "        plt.title(\"Cross-Entropy Loss: PCA=\"+str(k)+\" learning-rate=\"+str(learning_rate), fontsize=15)\n",
    "        plt.legend(loc = 'center left',bbox_to_anchor = (1,0.5))\n",
    "        \n",
    "        # calculate average and std of test accuracy\n",
    "        avg_test = round(np.mean(e_test[k]),2)\n",
    "        std_test = round(np.std(e_test[k]),2)\n",
    "        print(\"PCA=\"+str(k),str(avg_test*100)+\"%\", \"(\"+str(std_test)+\")\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot train error, holdout error for 5 runs with different learning rates\n",
    "# Calculate corresponding test accuracy\n",
    "def learning_rate(epoch,k_pca,e1,e2,encode,learning_rate):\n",
    "    e_train = {}\n",
    "    e_holdout = {}\n",
    "    e_test = {}\n",
    "    for i in range(5):\n",
    "        # choose train set, holdout set, test set \n",
    "        train_X,train_t,holdout_X, holdout_t,test_X, test_t = choose_set(e1,e2,encode,i)  \n",
    "\n",
    "        for rate in learning_rate:\n",
    "            if rate not in e_train:\n",
    "                e_train[rate] = []\n",
    "                e_holdout[rate] = []\n",
    "                e_test[rate] = []\n",
    "                \n",
    "            # transform images to k-dimension\n",
    "            pca = PCA(k = k_pca)\n",
    "            pca.fit(train_X)\n",
    "            train_x = [j[0] for j in [pca.transform(i) for i in train_X]]\n",
    "            holdout_x = [j[0] for j in [pca.transform(i) for i in holdout_X]]\n",
    "            test_x = [j[0] for j in [pca.transform(i) for i in test_X]]\n",
    "         \n",
    "            e_t, e_h, w_t = run_logistic(epoch,rate,train_x,train_t,holdout_x,holdout_t,test_x,test_t)\n",
    "            \n",
    "            # store train error, holdout error for each run\n",
    "            e_train[rate] += [e_t]\n",
    "            e_holdout[rate] += [e_h]\n",
    "            \n",
    "            # calculate test accuracy\n",
    "            test_y = logistic_reg(w_t, test_x)\n",
    "            e_test[rate] += [(test_y > 0.5) == test_t]\n",
    "\n",
    "    for r in learning_rate:\n",
    "        plt.figure(learning_rate.index(r))\n",
    "        \n",
    "        # calculate average of train error, holdout error for 10 epochs\n",
    "        avg_train = [np.mean([x[i] for x in e_train[r]]) for i in range(10)]\n",
    "        avg_holdout = [np.mean([x[i] for x in e_holdout[r]]) for i in range(10)]\n",
    "        \n",
    "        # calculate std of train error, holdout error for epoch 2,4,6,8\n",
    "        std_train = [np.std([x[i-1] for x in e_train[r]]) for i in [2,4,6,8]]\n",
    "        std_holdout = [np.std([x[i-1] for x in e_holdout[r]]) for i in [2,4,6,8]]\n",
    "    \n",
    "        # plot graphs\n",
    "        x = [i for i in range(1,11)]\n",
    "        plt.plot(x, avg_train, label = \"avg train error\")\n",
    "        plt.plot(x, avg_holdout, label = \"avg holdout error\")\n",
    "        plt.plot([2,4,6,8], std_train, label = \"std train error\")\n",
    "        plt.plot([2,4,6,8], std_holdout, label = \"std holdout error\")\n",
    "        plt.xticks(np.arange(1, 11, step=1))\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Cross-Entropy Loss\")\n",
    "        plt.title(\"Cross-Entropy Loss: PCA=\"+str(k_pca)+\" learning-rate=\"+str(r), fontsize=15)\n",
    "        plt.legend(loc = 'center left',bbox_to_anchor = (1,0.5))\n",
    "        \n",
    "        # calculate average and std of test accuracy\n",
    "        avg_test = np.mean(e_test[r])\n",
    "        std_test = round(np.std(e_test[r]),2)\n",
    "        print(\"learning rate=\"+str(r),str(avg_test*100)+\"%\", \"(\"+str(std_test)+\")\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# # Softmax regression\n",
    "\n",
    "\n",
    "# Choose images with the 6 emotions(no happy,neutral) from a subject\n",
    "# build its corresponding expected values using one hot encoding\n",
    "def subject_emo_softmax(subject,classifier,dataset = dict_label):\n",
    "    list1 = ['h','m','s','f','a','d']\n",
    "    x_set = []\n",
    "    t = []\n",
    "    for i in subject:\n",
    "        for l,img in dataset[i]:\n",
    "            x_set += [img]\n",
    "            t += [[int(i == list1.index(l[0])) for i in range(classifier)]]\n",
    "\n",
    "    return (np.array(x_set),np.array(t))\n",
    "\n",
    "\n",
    "# Choose a pair of subject as test set\n",
    "# randomly choose a pair of subjects as holdout set from the rest\n",
    "# then the remaining is train set \n",
    "def choose_set_softmax(index, classifier = 6, dataset = dict_label):\n",
    "    subject = list(dataset.keys())\n",
    "    \n",
    "    test = subject[2*index: 2*index+2]\n",
    "    test_x, test_t = subject_emo_softmax(test,classifier)\n",
    "    \n",
    "    rest = subject[:2*index] + subject[2*index+2:]\n",
    "    random.shuffle(rest)\n",
    "    \n",
    "    train = rest[:6]\n",
    "    train_x, train_t = subject_emo_softmax(train,classifier)\n",
    "    \n",
    "    holdout = rest[6:]\n",
    "    holdout_x, holdout_t = subject_emo_softmax(holdout,classifier)\n",
    "    \n",
    "    return train_x,train_t,holdout_x, holdout_t,test_x, test_t\n",
    "\n",
    "\n",
    "# Compute loss function for softmax regression\n",
    "def loss_softmax(t_kn, y_kn):\n",
    "    N = len(t_kn)\n",
    "    c = len(t_kn[0])\n",
    "    sumE = 0\n",
    "    for n in range(N):\n",
    "        for k in range(c):\n",
    "            sumE += -(t_kn[n][k])*np.log(y_kn[n][k])\n",
    "    return sumE/N/c\n",
    "\n",
    "\n",
    "# Compute predicted values for softmax regression\n",
    "def softmax_reg(w, x_kn):\n",
    "    size_N = len(x_kn)\n",
    "    d = len(w)\n",
    "    c = len(w[0])\n",
    "    a_kn = np.zeros([size_N,c])\n",
    "    y_kn = np.zeros([size_N,c])\n",
    "    \n",
    "    for n in range(size_N):\n",
    "        for k in range(c):\n",
    "            a_kn[n][k] = np.dot([w[i][k] for i in range(d)],x_kn[n])\n",
    "        for k in range(c):\n",
    "            y_kn[n][k] = np.exp(a_kn[n][k])/np.sum([np.exp(a_kn[n][j]) for j in range(c)])\n",
    "    return y_kn\n",
    "   \n",
    "\n",
    "# Compute the train error, holdout error and best weights for each run\n",
    "def run_softmax(epoch, learning_rate, train_x, train_t, holdout_x, holdout_t, test_x, test_t, mode = \"BGD\"):\n",
    "\n",
    "    N = len(train_x)\n",
    "    c = len(train_t[0])\n",
    "    d = len(train_x[0])\n",
    "    \n",
    "    e_t = []\n",
    "    e_h = []\n",
    "    error = 1\n",
    "    w_t = []\n",
    "\n",
    "    # weight vector initialization\n",
    "    w = np.zeros([d,c])\n",
    "    w_t = np.zeros([d,c])\n",
    "    \n",
    "    \n",
    "    for i in range(epoch):\n",
    "        \n",
    "        # use batch gradient descent to update weight vector and record train error\n",
    "        if mode == \"BGD\":\n",
    "            train_y = softmax_reg(w, train_x)\n",
    "            e_t += [loss_softmax(train_t, train_y)]\n",
    "            sumE = np.zeros([d, c])\n",
    "            for n in range(N):\n",
    "                m = [[(train_t[n][k] - train_y[n][k])*train_x[n][j] for k in range(c)] for j in range(d)]            \n",
    "                sumE += m\n",
    "            w = w + learning_rate/N/c*sumE    \n",
    "            \n",
    "        # use stochastic gradient descent to update weight vector and record train error\n",
    "        else:\n",
    "            P = [i for i in range(N)]\n",
    "            e_epoch = []\n",
    "            random.shuffle(P)\n",
    "            for n in range(N):\n",
    "                train_y = softmax_reg(w, [train_x[P[n]]])\n",
    "                \n",
    "                e_epoch += [loss_softmax([train_t[P[n]]], train_y)]\n",
    "                m = [[(train_t[P[n]][k] - train_y[0][k])*train_x[P[n]][j] for k in range(c)] for j in range(d)]\n",
    "                w = w + np.array(m)*learning_rate/c\n",
    "#                 train_y[P[n]] = softmax_reg(w, [train_x[P[n]]])\n",
    "            e_t += [np.mean(e_epoch)]\n",
    "        \n",
    "        # record holdout error and best weights\n",
    "        holdout_y = softmax_reg(w, holdout_x)\n",
    "        e_h += [loss_softmax(holdout_t, holdout_y)] \n",
    "        \n",
    "        if e_h[-1] <= error:\n",
    "            error = e_h[-1]\n",
    "            w_t = w\n",
    "    \n",
    "    return e_t, e_h, w_t\n",
    "\n",
    "\n",
    "# Plot train error, holdout error for 5 runs with different PCAs\n",
    "# Calculate corresponding confusion matrix\n",
    "def training_proc_softmax(epoch, kd,learning_rate):\n",
    "    e_train = {}\n",
    "    e_holdout = {}\n",
    "    e_test = {}\n",
    "    w = {}\n",
    "    confusion = {}\n",
    "    \n",
    "    for i in range(5):\n",
    "        # choose train set, holdout set, test set \n",
    "        train_X,train_t,holdout_X, holdout_t,test_X, test_t = choose_set_softmax(i)  \n",
    "       \n",
    "        for k_pca in kd:\n",
    "            if k_pca not in e_train:\n",
    "                e_train[k_pca] = []\n",
    "                e_holdout[k_pca] = []\n",
    "                e_test[k_pca] = []\n",
    "                w[k_pca] = []\n",
    "                confusion[k_pca] = np.zeros([6,6])\n",
    "                \n",
    "            # transform images to k-dimension\n",
    "            pca = PCA(k = k_pca)\n",
    "            pca.fit(train_X)\n",
    "            train_x = [j[0] for j in [pca.transform(i) for i in train_X]]\n",
    "            holdout_x = [j[0] for j in [pca.transform(i) for i in holdout_X]]\n",
    "            test_x = [j[0] for j in [pca.transform(i) for i in test_X]]\n",
    "\n",
    "            e_t, e_h, w_t = run_softmax(epoch,learning_rate,train_x,train_t,holdout_x,holdout_t,test_x,test_t)\n",
    "    \n",
    "            # store train error, holdout error for each run\n",
    "            e_train[k_pca] += [e_t]\n",
    "            e_holdout[k_pca] += [e_h]\n",
    "            \n",
    "            # calculate test accuracy\n",
    "            test_y = softmax_reg(w_t, test_x)\n",
    "            for t in range(len(test_y)):\n",
    "                maxt = max(test_y[t])\n",
    "                test = list(test_y[t]).index(maxt)\n",
    "                correct = list(test_t[t]).index(1)\n",
    "                confusion[k_pca][correct][test] += 1\n",
    "            \n",
    "            w[k_pca] += [w_t]\n",
    "    \n",
    "    # calculate confusion matrix\n",
    "    for k_pca in kd:\n",
    "        confusion[k_pca] = np.array([c/sum(c)for c in confusion[k_pca]])\n",
    "        print(\"confusion matrix: \")\n",
    "        print(confusion[k_pca])\n",
    "    \n",
    "    for k in kd:\n",
    "        plt.figure(kd.index(k))\n",
    "        \n",
    "        # calculate average of train error, holdout error for 20 epochs\n",
    "        avg_train = [np.mean([x[i-1] for x in e_train[k]]) for i in range(1,21)]\n",
    "        avg_holdout = [np.mean([x[i-1] for x in e_holdout[k]]) for i in range(1,21)]\n",
    "        \n",
    "        # calculate std of train error, holdout error for epoch 5,10,15,20\n",
    "        std_train = [np.std([x[i-1] for x in e_train[k]]) for i in [5,10,15,20]]\n",
    "        std_holdout = [np.std([x[i-1] for x in e_holdout[k]]) for i in [5,10,15,20]]\n",
    "        \n",
    "        # plot graphs\n",
    "        x = [i for i in range(1,21)]\n",
    "        plt.plot(x, avg_train, label = \"train error\")\n",
    "        plt.plot(x, avg_holdout, label = \"holdout error\")\n",
    "        plt.plot([5,10,15,20], std_train, label = \"std train error\")\n",
    "        plt.plot([5,10,15,20], std_holdout, label = \"std holdout error\")\n",
    "        plt.xticks(np.arange(1, 21, step=1))\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Cross-Entropy Loss\")\n",
    "        plt.title(\"Cross-Entropy Loss: PCA=\"+str(k)+\" learning-rate=\"+str(learning_rate), fontsize=15)\n",
    "        plt.legend(loc = 'center left',bbox_to_anchor = (1,0.5))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot train error curves for batch gradient descent and for stochastic gradient descent\n",
    "def BGD_SGD(epoch, kd,learning_rate):\n",
    "    e_train = {}\n",
    "    e_holdout = {}\n",
    "    e_test = {}\n",
    "    \n",
    "    for i in range(5):\n",
    "        # choose train set, holdout set, test set\n",
    "        train_X,train_t,holdout_X, holdout_t,test_X, test_t = choose_set_softmax(i)  \n",
    "       \n",
    "        # choose mode batch gradient descent or stochastic gradient descent\n",
    "        for mode in [\"BGD\",\"SGD\"]:\n",
    "            if mode not in e_train:\n",
    "                e_train[mode] = []\n",
    "                e_holdout[mode] = []\n",
    "                e_test[mode] = []\n",
    "                \n",
    "            # transform images to k-dimension\n",
    "            pca = PCA(k = kd)\n",
    "            pca.fit(train_X)\n",
    "            train_x = [j[0] for j in [pca.transform(i) for i in train_X]]\n",
    "            holdout_x = [j[0] for j in [pca.transform(i) for i in holdout_X]]\n",
    "            test_x = [j[0] for j in [pca.transform(i) for i in test_X]]\n",
    "\n",
    "            e_t, e_h, w_t = run_softmax(epoch, learning_rate,train_x,train_t,holdout_x,holdout_t,test_x,test_t, mode)\n",
    "            \n",
    "            # store train error, holdout error for each run\n",
    "            e_train[mode] += [e_t]\n",
    "            e_holdout[mode] += [e_h]\n",
    "\n",
    "\n",
    "    for mode in [\"BGD\",\"SGD\"]:\n",
    "        # calculate average of train error for 20 epochs\n",
    "        avg_train = [np.mean([x[i] for x in e_train[mode]]) for i in range(epoch)]\n",
    "\n",
    "        # plot graphs\n",
    "        x = [i for i in range(1,epoch+1)]\n",
    "        plt.plot(x, avg_train, label = mode,markersize=5)\n",
    "        plt.xticks(np.arange(1, 21, step=1))\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Cross-Entropy Loss\")\n",
    "        plt.title(\"Cross-Entropy Loss: PCA=\"+str(kd)+\" learning-rate=\"+str(learning_rate), fontsize=15)\n",
    "        plt.legend(loc = 'upper right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# linear scale a vector to make the minimum value is 0, maximum value is 255\n",
    "def linearscale(vec):\n",
    "    minV = np.min(vec)\n",
    "    maxV = np.max(vec)\n",
    "    a = 255/(maxV-minV)\n",
    "    b = -minV*a\n",
    "    return [a*v+b for v in vec]\n",
    "\n",
    "\n",
    "# visualization of weight\n",
    "def visualize(epoch,kd,learning_rate):\n",
    "    w = []\n",
    "    list1 = ['happy with teeth','maudlin','surprise','fear','angry','disgust']\n",
    "    inverse = {}\n",
    "    \n",
    "    for i in range(5):\n",
    "        # choose train set, holdout set, test set\n",
    "        train_X,train_t,holdout_X, holdout_t,test_X, test_t = choose_set_softmax(i)      \n",
    "      \n",
    "        # transform images to k-dimension\n",
    "        pca = PCA(k = kd)\n",
    "        pca.fit(train_X)\n",
    "        train_x = [j[0] for j in [pca.transform(i) for i in train_X]]\n",
    "        holdout_x = [j[0] for j in [pca.transform(i) for i in holdout_X]]\n",
    "        test_x = [j[0] for j in [pca.transform(i) for i in test_X]]\n",
    "\n",
    "        e_t, e_h, w_t = run_softmax(epoch,learning_rate,train_x,train_t,holdout_x,holdout_t,test_x,test_t)\n",
    "        \n",
    "        # for each emotion, transform the corresponding weight to the original image representation\n",
    "        for l in list1:\n",
    "            if (l not in inverse):\n",
    "                inverse[l] = []\n",
    "            k = list1.index(l)\n",
    "            w_k = [cate[k] for cate in w_t]\n",
    "            inverse[l] += [pca.inverse_transform([[w_k]])]\n",
    "    \n",
    "    for l in list1:\n",
    "        inverse[l] = np.mean(inverse[l], axis = 0)\n",
    "        # linear scale\n",
    "        scale = linearscale(inverse[l])\n",
    "        plt.title(l)\n",
    "        plt.imshow(np.array(scale), cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# # Extra Credit\n",
    "\n",
    "# Choose images with the specified emotions from all the 10 subjects\n",
    "# build its corresponding expected values using one hot encoding\n",
    "def subject_emo_ec(emotion,classifier,dataset = dict_label_ec):\n",
    "    list1 = ['018', '027', '036', '037', '041', '043', '044', '048ng', '049', '050']\n",
    "\n",
    "    x_set = []\n",
    "    t = []\n",
    "    for i in emotion:\n",
    "        for l,img in dataset[i]:\n",
    "            x_set += [img]\n",
    "            t += [[int(j == list1.index(l)) for j in range(classifier)]]\n",
    "    return (np.array(x_set),np.array(t))\n",
    "\n",
    "\n",
    "# Choose one emotion of all 10 subjects as test set\n",
    "# randomly choose anothor emotion of all 10 subjects as holdout set from the rest\n",
    "# then the remaining is train set \n",
    "def choose_set_ec(index, classifier = 6):\n",
    "    emotions = ['h','m','s','f','a','d']\n",
    "    \n",
    "    test = emotions[index : index+1]\n",
    "    test_x, test_t = subject_emo_ec(test,classifier)\n",
    "    \n",
    "    rest = emotions[:index] + emotions[index+1:]\n",
    "    random.shuffle(rest)\n",
    "    \n",
    "    holdout = rest[:1]\n",
    "    holdout_x, holdout_t = subject_emo_ec(holdout,classifier)\n",
    "\n",
    "    train = rest[1:]\n",
    "    train_x, train_t = subject_emo_ec(train,classifier)\n",
    "\n",
    "    return train_x,train_t,holdout_x, holdout_t,test_x, test_t\n",
    "\n",
    "\n",
    "# Plot train error, holdout error for 6 runs with different PCAs\n",
    "def training_proc_ec(epoch, kd,learning_rate):\n",
    "    e_train = {}\n",
    "    e_holdout = {}\n",
    "    e_test = {}\n",
    "    w = {}\n",
    "    \n",
    "    for i in range(6):\n",
    "        # choose train set, holdout set, test set\n",
    "        train_X,train_t,holdout_X, holdout_t,test_X, test_t = choose_set_ec(i,10)  \n",
    "       \n",
    "        for k_pca in kd:\n",
    "            if k_pca not in e_train:\n",
    "                e_train[k_pca] = []\n",
    "                e_holdout[k_pca] = []\n",
    "                e_test[k_pca] = []\n",
    "                w[k_pca] = []\n",
    "            \n",
    "            # transform images to k-dimension\n",
    "            pca = PCA(k = k_pca)\n",
    "            pca.fit(train_X)\n",
    "            train_x = [j[0] for j in [pca.transform(i) for i in train_X]]\n",
    "            holdout_x = [j[0] for j in [pca.transform(i) for i in holdout_X]]\n",
    "            test_x = [j[0] for j in [pca.transform(i) for i in test_X]]\n",
    "\n",
    "            e_t, e_h, w_t = run_softmax(epoch,learning_rate,train_x,train_t,holdout_x,holdout_t,test_x,test_t)\n",
    "    \n",
    "            # store train error, holdout error for each run\n",
    "            e_train[k_pca] += [e_t]\n",
    "            e_holdout[k_pca] += [e_h]\n",
    "            \n",
    "            test_y = softmax_reg(w_t, test_x)            \n",
    "            w[k_pca] += [w_t]\n",
    " \n",
    "    for k in kd:\n",
    "        plt.figure(kd.index(k))\n",
    "        \n",
    "        # calculate average of train error for 20 epochs\n",
    "        avg_train = [np.mean([x[i-1] for x in e_train[k]]) for i in range(1,21)]\n",
    "        avg_holdout = [np.mean([x[i-1] for x in e_holdout[k]]) for i in range(1,21)]\n",
    "        \n",
    "        # calculate std of train error for epoch 5,10,15,20\n",
    "        std_train = [np.std([x[i-1] for x in e_train[k]]) for i in [5,10,15,20]]\n",
    "        std_holdout = [np.std([x[i-1] for x in e_holdout[k]]) for i in [5,10,15,20]]\n",
    "        \n",
    "        # plot graphs\n",
    "        x = [i for i in range(1,21)]\n",
    "        plt.plot(x, avg_train, label = \"train error\")\n",
    "        plt.plot(x, avg_holdout, label = \"holdout error\")\n",
    "        plt.plot([5,10,15,20], std_train, label = \"std train error\")\n",
    "        plt.plot([5,10,15,20], std_holdout, label = \"std holdout error\")\n",
    "        plt.xticks(np.arange(1, 21, step=1))\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Cross-Entropy Loss\")\n",
    "        plt.title(\"Cross-Entropy Loss: PCA=\"+str(k)+\" learning-rate=\"+str(learning_rate), fontsize=15)\n",
    "        plt.legend(loc = 'center left',bbox_to_anchor = (1,0.5))\n",
    "    plt.show()\n",
    "   \n",
    "\n",
    "# visualization of weight\n",
    "def visualize_ec(epoch, kd, learning_rate):\n",
    "    w = []\n",
    "    list1 = ['018', '027', '036', '037', '041', '043', '044', '048ng', '049', '050']\n",
    "\n",
    "    inverse = {}\n",
    "    \n",
    "    for i in range(6):\n",
    "        # choose train set, holdout set, test set\n",
    "        train_X,train_t,holdout_X, holdout_t,test_X, test_t = choose_set_ec(i,10)      \n",
    "           \n",
    "        # transform images to k-dimension\n",
    "        pca = PCA(k = kd)\n",
    "        pca.fit(train_X)\n",
    "        train_x = [j[0] for j in [pca.transform(i) for i in train_X]]\n",
    "        holdout_x = [j[0] for j in [pca.transform(i) for i in holdout_X]]\n",
    "        test_x = [j[0] for j in [pca.transform(i) for i in test_X]]\n",
    "\n",
    "        e_t, e_h, w_t = run_softmax(epoch,learning_rate,train_x,train_t,holdout_x,holdout_t,test_x,test_t)\n",
    "\n",
    "        # for each subject, transform the corresponding weight to the original image representation\n",
    "        for l in list1:\n",
    "            if (l not in inverse):\n",
    "                inverse[l] = []\n",
    "            k = list1.index(l)\n",
    "            w_k = [cate[k] for cate in w_t]\n",
    "            inverse[l] += [pca.inverse_transform([[w_k]])]\n",
    "        \n",
    "    for l in list1:\n",
    "        inverse[l] = np.mean(inverse[l], axis = 0)\n",
    "        # linear scale\n",
    "        scale = linearscale(inverse[l])\n",
    "        plt.title(l)\n",
    "        plt.imshow(np.array(scale), cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"*****1.(b)*****\")\n",
    "    # display 6 different emotions from one subject\n",
    "    for i in ['n','m','s','f','a','d']:\n",
    "        for l in labels:\n",
    "            if '018_'+i in l:\n",
    "                plt.imshow(images[labels.index(l)], cmap='gray')\n",
    "                plt.savefig('018'+str(i)+'.png')\n",
    "                plt.show()\n",
    "    \n",
    "    print(\"*****1.(c)*****\")\n",
    "    # display first 6 eigenvectors\n",
    "    # k is the number of principal components \n",
    "    pca = PCA(k=50)\n",
    "    # choose training images\n",
    "    pca.fit(np.array(images[:]))\n",
    "    pca.display('./1c_pca_display.png', 6)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"*****2.(b)*****\")\n",
    "    training_proc(10, [1,2,4,8],'h','m',1,2)\n",
    "    learning_rate(10, 8,'h','m',1,[0.01,3,10])\n",
    "    \n",
    "    print(\"*****2.(c)*****\")\n",
    "    learning_rate(10, 8,'f','s',1,[3]) \n",
    "    \n",
    "    print(\"*****3.(a)*****\")\n",
    "    training_proc_softmax(20, [10,20,30], 3)\n",
    "    training_proc_softmax(20, [10,20,30], 5)\n",
    "    training_proc_softmax(20, [i for i in range(5,35,5)], 3)\n",
    "    \n",
    "    print(\"*****3.(b)*****\")\n",
    "    BGD_SGD(20, 30, 5)\n",
    "    print(\"*****3.(c)*****\")\n",
    "    visualize(20,30,5)\n",
    "    \n",
    "    print(\"*****3.(d)*****\")\n",
    "    training_proc_ec(20, [30], 5)\n",
    "    training_proc_ec(20, [30], 10)\n",
    "    training_proc_ec(20, [30], 20)\n",
    "    visualize_ec(20,30,20)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
